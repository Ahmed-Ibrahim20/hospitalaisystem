"""
Model Monitoring and Performance Tracking
Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
"""

import pandas as pd
import numpy as np
from datetime import datetime
import json
import os
from pathlib import Path


class ModelMonitor:
    """
    Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
    - Feature Drift Detection
    - Performance Tracking
    - Prediction Logging
    """
    
    def __init__(self, log_dir='logs/predictions'):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.predictions_log = []
        self.performance_metrics = []
        
    def log_prediction(self, input_data, prediction, probability, patient_id=None):
        """
        ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤
        
        Parameters:
        -----------
        input_data : dict
            Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        prediction : int
            Ø§Ù„ØªÙ†Ø¨Ø¤
        probability : float
            Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
        patient_id : str
            Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø±ÙŠØ¶ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ - Ù„Ù„ØªØªØ¨Ø¹ ÙÙ‚Ø·)
        """
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'patient_id': patient_id,
            'prediction': int(prediction),
            'probability': float(probability),
            'input_features': {
                k: float(v) if isinstance(v, (int, float, np.number)) else v
                for k, v in input_data.items()
            }
        }
        
        self.predictions_log.append(log_entry)
        
        # Ø­ÙØ¸ ÙƒÙ„ 100 ØªÙ†Ø¨Ø¤
        if len(self.predictions_log) >= 100:
            self._save_predictions()
    
    def _save_predictions(self):
        """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        if not self.predictions_log:
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = self.log_dir / f'predictions_{timestamp}.json'
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(self.predictions_log, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(self.predictions_log)} ØªÙ†Ø¨Ø¤ ÙÙŠ: {log_file}")
        self.predictions_log = []
    
    def detect_feature_drift(self, current_data, reference_data, threshold=0.1):
        """
        ÙƒØ´Ù Feature Drift
        
        Parameters:
        -----------
        current_data : DataFrame
            Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        reference_data : DataFrame
            Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© (Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
        threshold : float
            Ø¹ØªØ¨Ø© Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
            
        Returns:
        --------
        dict : ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
        """
        drift_report = {}
        
        for col in current_data.columns:
            if col in reference_data.columns:
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±Ù‚ ÙÙŠ Ø§Ù„Ù…ØªÙˆØ³Ø·
                current_mean = current_data[col].mean()
                reference_mean = reference_data[col].mean()
                
                if reference_mean != 0:
                    drift = abs(current_mean - reference_mean) / abs(reference_mean)
                else:
                    drift = 0
                
                drift_report[col] = {
                    'current_mean': float(current_mean),
                    'reference_mean': float(reference_mean),
                    'drift': float(drift),
                    'drifted': drift > threshold
                }
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù†Ø­Ø±ÙØ©
        drifted_features = sum(1 for v in drift_report.values() if v['drifted'])
        
        return {
            'timestamp': datetime.now().isoformat(),
            'total_features': len(drift_report),
            'drifted_features': drifted_features,
            'drift_percentage': (drifted_features / len(drift_report)) * 100,
            'details': drift_report
        }
    
    def track_performance(self, y_true, y_pred, y_proba):
        """
        ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡
        
        Parameters:
        -----------
        y_true : array
            Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
        y_pred : array
            Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        y_proba : array
            Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        """
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score,
            f1_score, roc_auc_score
        )
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'accuracy': float(accuracy_score(y_true, y_pred)),
            'precision': float(precision_score(y_true, y_pred, zero_division=0)),
            'recall': float(recall_score(y_true, y_pred, zero_division=0)),
            'f1_score': float(f1_score(y_true, y_pred, zero_division=0)),
            'roc_auc': float(roc_auc_score(y_true, y_proba))
        }
        
        self.performance_metrics.append(metrics)
        
        return metrics
    
    def generate_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        report = {
            'generated_at': datetime.now().isoformat(),
            'total_predictions': len(self.predictions_log),
            'performance_history': self.performance_metrics[-10:]  # Ø¢Ø®Ø± 10 Ù‚ÙŠØ§Ø³Ø§Øª
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_file = self.log_dir / 'monitoring_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_file}")
        
        return report
    
    def alert_if_degraded(self, current_metrics, baseline_metrics, threshold=0.05):
        """
        Ø¥Ù†Ø´Ø§Ø¡ ØªÙ†Ø¨ÙŠÙ‡ Ø¥Ø°Ø§ Ø§Ù†Ø®ÙØ¶ Ø§Ù„Ø£Ø¯Ø§Ø¡
        
        Parameters:
        -----------
        current_metrics : dict
            Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        baseline_metrics : dict
            Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        threshold : float
            Ø¹ØªØ¨Ø© Ø§Ù„Ø§Ù†Ø®ÙØ§Ø¶
            
        Returns:
        --------
        bool : Ù‡Ù„ ÙŠÙˆØ¬Ø¯ ØªØ¯Ù‡ÙˆØ±ØŸ
        """
        degradation = {}
        
        for metric in ['accuracy', 'precision', 'recall', 'roc_auc']:
            if metric in current_metrics and metric in baseline_metrics:
                current = current_metrics[metric]
                baseline = baseline_metrics[metric]
                
                diff = baseline - current
                degradation[metric] = {
                    'current': current,
                    'baseline': baseline,
                    'difference': diff,
                    'degraded': diff > threshold
                }
        
        is_degraded = any(v['degraded'] for v in degradation.values())
        
        if is_degraded:
            print("\nâš ï¸ ØªØ­Ø°ÙŠØ±: Ø§Ù†Ø®ÙØ§Ø¶ ÙÙŠ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬!")
            for metric, info in degradation.items():
                if info['degraded']:
                    print(f"   {metric}: {info['current']:.4f} (ÙƒØ§Ù† {info['baseline']:.4f})")
        
        return is_degraded


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    print("="*80)
    print("ğŸ“Š Model Monitoring System")
    print("="*80)
    
    monitor = ModelMonitor()
    
    # Ù…Ø­Ø§ÙƒØ§Ø© ØªÙ†Ø¨Ø¤Ø§Øª
    for i in range(5):
        sample_input = {
            'HighBP': np.random.randint(0, 2),
            'BMI': np.random.uniform(20, 35),
            'Age': np.random.randint(1, 13)
        }
        
        prediction = np.random.randint(0, 2)
        probability = np.random.uniform(0.3, 0.9)
        
        monitor.log_prediction(sample_input, prediction, probability, f"patient_{i}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ±
    report = monitor.generate_report()
    
    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±!")
