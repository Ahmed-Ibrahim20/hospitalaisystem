"""
Advanced ML Models with SHAP Explainability
Ù†Ù…Ø§Ø°Ø¬ Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ ØªÙØ³ÙŠØ± SHAP
"""

import pandas as pd
import numpy as np
import joblib
import os
import json
from datetime import datetime
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_curve, average_precision_score, roc_curve
)
from sklearn.calibration import calibration_curve
import warnings
warnings.filterwarnings('ignore')

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost ØºÙŠØ± Ù…Ø«Ø¨Øª")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBM ØºÙŠØ± Ù…Ø«Ø¨Øª")

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("âš ï¸ SHAP ØºÙŠØ± Ù…Ø«Ø¨Øª")

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from preprocessing import load_and_prepare_data, create_preprocessing_pipeline
from advanced_features import MedicalFeatureEngineer


class AdvancedDiabetesPredictor:
    """
    Ù†Ù…ÙˆØ°Ø¬ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø³ÙƒØ±ÙŠ Ù…Ø¹:
    - XGBoost / LightGBM / Ensemble
    - SHAP Explainability
    - Model Calibration
    - Cross-validation
    """
    
    def __init__(self, model_type='xgboost', use_advanced_features=True):
        """
        Parameters:
        -----------
        model_type : str
            'xgboost', 'lightgbm', 'random_forest', 'ensemble'
        use_advanced_features : bool
            Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        """
        self.model_type = model_type
        self.use_advanced_features = use_advanced_features
        self.model = None
        self.preprocessing_pipeline = None
        self.feature_engineer = None
        self.feature_names = None
        self.shap_explainer = None
        self.training_history = {}
        
    def _create_model(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if self.model_type == 'xgboost' and XGBOOST_AVAILABLE:
            return xgb.XGBClassifier(
                n_estimators=300,
                max_depth=6,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                min_child_weight=3,
                gamma=0.1,
                scale_pos_weight=5,  # Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ imbalance
                random_state=42,
                n_jobs=-1,
                eval_metric='logloss'
            )
        elif self.model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            return lgb.LGBMClassifier(
                n_estimators=300,
                max_depth=6,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                min_child_samples=20,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )
        elif self.model_type == 'ensemble':
            # Ensemble Ù…Ù† Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬
            models = []
            
            if XGBOOST_AVAILABLE:
                models.append(('xgb', self._create_xgb()))
            if LIGHTGBM_AVAILABLE:
                models.append(('lgb', self._create_lgb()))
            models.append(('rf', self._create_rf()))
            
            return VotingClassifier(estimators=models, voting='soft', n_jobs=-1)
        else:
            # Random Forest ÙƒÙ€ fallback
            return RandomForestClassifier(
                n_estimators=300,
                max_depth=15,
                min_samples_split=10,
                min_samples_leaf=4,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            )
    
    def _create_xgb(self):
        """Ø¥Ù†Ø´Ø§Ø¡ XGBoost"""
        return xgb.XGBClassifier(
            n_estimators=200, max_depth=6, learning_rate=0.05,
            scale_pos_weight=5, random_state=42, n_jobs=-1
        )
    
    def _create_lgb(self):
        """Ø¥Ù†Ø´Ø§Ø¡ LightGBM"""
        return lgb.LGBMClassifier(
            n_estimators=200, max_depth=6, learning_rate=0.05,
            class_weight='balanced', random_state=42, n_jobs=-1, verbose=-1
        )
    
    def _create_rf(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Random Forest"""
        return RandomForestClassifier(
            n_estimators=200, max_depth=15, class_weight='balanced',
            random_state=42, n_jobs=-1
        )
    
    def train(self, X_train, y_train, X_val=None, y_val=None, use_cv=True):
        """
        ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Cross-Validation
        """
        print("\n" + "="*80)
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
        print("="*80)
        
        # 1. Preprocessing
        print("\n1ï¸âƒ£ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        self.preprocessing_pipeline = create_preprocessing_pipeline(scale_features=False)
        X_train_processed = self.preprocessing_pipeline.fit_transform(X_train)
        
        # 2. Advanced Features
        if self.use_advanced_features:
            print("2ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©...")
            self.feature_engineer = MedicalFeatureEngineer()
            X_train_processed = self.feature_engineer.fit_transform(
                pd.DataFrame(X_train_processed, columns=X_train.columns)
            )
        
        # Ø­ÙØ¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        if isinstance(X_train_processed, pd.DataFrame):
            self.feature_names = X_train_processed.columns.tolist()
            X_train_processed = X_train_processed.values
        else:
            self.feature_names = [f"feature_{i}" for i in range(X_train_processed.shape[1])]
        
        print(f"   âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª: {X_train_processed.shape[1]}")
        
        # 3. Cross-Validation
        if use_cv:
            print("\n3ï¸âƒ£ Cross-Validation...")
            self.model = self._create_model()
            
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            cv_scores = cross_val_score(
                self.model, X_train_processed, y_train,
                cv=cv, scoring='roc_auc', n_jobs=-1
            )
            
            print(f"   CV ROC-AUC Scores: {cv_scores}")
            print(f"   Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
            
            self.training_history['cv_scores'] = cv_scores.tolist()
            self.training_history['cv_mean'] = float(cv_scores.mean())
            self.training_history['cv_std'] = float(cv_scores.std())
        
        # 4. Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        print("\n4ï¸âƒ£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        self.model = self._create_model()
        
        start_time = datetime.now()
        self.model.fit(X_train_processed, y_train)
        training_time = (datetime.now() - start_time).total_seconds()
        
        print(f"   âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ {training_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        
        # 5. Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        train_score = self.model.score(X_train_processed, y_train)
        print(f"\n5ï¸âƒ£ Training Accuracy: {train_score:.4f}")
        
        if X_val is not None and y_val is not None:
            X_val_processed = self.preprocessing_pipeline.transform(X_val)
            if self.use_advanced_features:
                X_val_processed = self.feature_engineer.transform(
                    pd.DataFrame(X_val_processed, columns=X_val.columns)
                )
                if isinstance(X_val_processed, pd.DataFrame):
                    X_val_processed = X_val_processed.values
            
            val_score = self.model.score(X_val_processed, y_val)
            print(f"   Validation Accuracy: {val_score:.4f}")
        
        # 6. SHAP Explainer
        if SHAP_AVAILABLE:
            print("\n6ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ SHAP Explainer...")
            try:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù„ØªØ³Ø±ÙŠØ¹ SHAP
                sample_size = min(1000, X_train_processed.shape[0])
                sample_indices = np.random.choice(
                    X_train_processed.shape[0], sample_size, replace=False
                )
                X_sample = X_train_processed[sample_indices]
                
                if self.model_type in ['xgboost', 'lightgbm']:
                    self.shap_explainer = shap.TreeExplainer(self.model)
                else:
                    self.shap_explainer = shap.KernelExplainer(
                        self.model.predict_proba, X_sample
                    )
                print("   âœ… SHAP Explainer Ø¬Ø§Ù‡Ø²")
            except Exception as e:
                print(f"   âš ï¸ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ SHAP: {str(e)}")
        
        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.training_history.update({
            'model_type': self.model_type,
            'training_samples': len(X_train),
            'features_count': X_train_processed.shape[1],
            'training_time_seconds': training_time,
            'use_advanced_features': self.use_advanced_features,
            'timestamp': datetime.now().isoformat()
        })
        
        print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
    
    def evaluate(self, X_test, y_test, verbose=True):
        """ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù…Ø¹ Calibration"""
        if self.model is None:
            raise ValueError("ÙŠØ¬Ø¨ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹!")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_test_processed = self.preprocessing_pipeline.transform(X_test)
        if self.use_advanced_features:
            X_test_processed = self.feature_engineer.transform(
                pd.DataFrame(X_test_processed, columns=X_test.columns)
            )
            if isinstance(X_test_processed, pd.DataFrame):
                X_test_processed = X_test_processed.values
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        y_pred = self.model.predict(X_test_processed)
        y_proba = self.model.predict_proba(X_test_processed)[:, 1]
        
        # Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        results = {
            'accuracy': float(self.model.score(X_test_processed, y_test)),
            'roc_auc': float(roc_auc_score(y_test, y_proba)),
            'average_precision': float(average_precision_score(y_test, y_proba)),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
            'classification_report': classification_report(y_test, y_pred, output_dict=True)
        }
        
        # Calibration
        try:
            prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10)
            results['calibration'] = {
                'prob_true': prob_true.tolist(),
                'prob_pred': prob_pred.tolist()
            }
        except:
            pass
        
        if verbose:
            print("\n" + "="*80)
            print("ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
            print("="*80)
            print(f"\nğŸ¯ Accuracy: {results['accuracy']:.4f}")
            print(f"ğŸ“ˆ ROC-AUC: {results['roc_auc']:.4f}")
            print(f"ğŸ“Š Average Precision: {results['average_precision']:.4f}")
            
            print("\nğŸ“‹ Classification Report:")
            print(classification_report(y_test, y_pred))
            
            print("\nğŸ”¢ Confusion Matrix:")
            cm = results['confusion_matrix']
            print(f"   TN: {cm[0][0]:6d}  |  FP: {cm[0][1]:6d}")
            print(f"   FN: {cm[1][0]:6d}  |  TP: {cm[1][1]:6d}")
        
        return results
    
    def explain_prediction(self, X, top_n=5):
        """
        ØªÙØ³ÙŠØ± Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SHAP
        
        Returns:
        --------
        dict: Ø§Ù„ØªÙ†Ø¨Ø¤ + SHAP values + Ø£Ù‡Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª
        """
        if self.model is None:
            raise ValueError("ÙŠØ¬Ø¨ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹!")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if isinstance(X, dict):
            X = pd.DataFrame([X])
        
        X_processed = self.preprocessing_pipeline.transform(X)
        if self.use_advanced_features:
            X_processed = self.feature_engineer.transform(
                pd.DataFrame(X_processed, columns=X.columns)
            )
            if isinstance(X_processed, pd.DataFrame):
                X_processed = X_processed.values
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        prediction = self.model.predict(X_processed)[0]
        probability = self.model.predict_proba(X_processed)[0]
        
        result = {
            'prediction': int(prediction),
            'probability': float(probability[1]),
            'confidence': float(max(probability))
        }
        
        # SHAP Explanation
        if self.shap_explainer is not None and SHAP_AVAILABLE:
            try:
                shap_values = self.shap_explainer.shap_values(X_processed)
                
                # Ø£Ø®Ø° SHAP values Ù„Ù„ÙØ¦Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
                if isinstance(shap_values, list):
                    shap_values = shap_values[1]
                
                # Ø£Ù‡Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª
                shap_abs = np.abs(shap_values[0])
                top_indices = np.argsort(shap_abs)[-top_n:][::-1]
                
                top_features = []
                for idx in top_indices:
                    top_features.append({
                        'feature': self.feature_names[idx],
                        'shap_value': float(shap_values[0][idx]),
                        'impact': 'positive' if shap_values[0][idx] > 0 else 'negative'
                    })
                
                result['shap_explanation'] = top_features
            except Exception as e:
                result['shap_explanation'] = f"Error: {str(e)}"
        
        return result
    
    def save(self, model_path='models/saved/advanced_diabetes_model.pkl'):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if self.model is None:
            raise ValueError("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø­ÙØ¸!")
        
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        model_data = {
            'model': self.model,
            'preprocessing_pipeline': self.preprocessing_pipeline,
            'feature_engineer': self.feature_engineer,
            'feature_names': self.feature_names,
            'model_type': self.model_type,
            'use_advanced_features': self.use_advanced_features,
            'training_history': self.training_history,
            'shap_explainer': self.shap_explainer
        }
        
        joblib.dump(model_data, model_path)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {model_path}")
        
        # Ø­ÙØ¸ metadata
        metadata_path = model_path.replace('.pkl', '_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            # Ø¥Ø²Ø§Ù„Ø© shap_explainer Ù…Ù† metadata (Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù€ JSON)
            metadata = {k: v for k, v in self.training_history.items() 
                       if k != 'shap_explainer'}
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Metadata ÙÙŠ: {metadata_path}")
    
    @classmethod
    def load(cls, model_path='models/saved/advanced_diabetes_model.pkl'):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {model_path}")
        
        model_data = joblib.load(model_path)
        
        predictor = cls(
            model_type=model_data['model_type'],
            use_advanced_features=model_data['use_advanced_features']
        )
        predictor.model = model_data['model']
        predictor.preprocessing_pipeline = model_data['preprocessing_pipeline']
        predictor.feature_engineer = model_data.get('feature_engineer')
        predictor.feature_names = model_data['feature_names']
        predictor.training_history = model_data['training_history']
        predictor.shap_explainer = model_data.get('shap_explainer')
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {model_path}")
        return predictor


# Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
if __name__ == "__main__":
    print("="*80)
    print("ğŸ¥ Advanced Diabetes Prediction with SHAP")
    print("="*80)
    
    data_path = "../DataSet/diabetes_binary_5050split_health_indicators_BRFSS2015.csv"
    
    if not os.path.exists(data_path):
        print(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {data_path}")
        exit(1)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("\nğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    X_train, X_test, y_train, y_test = load_and_prepare_data(
        data_path, test_size=0.2, random_state=42
    )
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ÙŠÙ†Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹
    sample_size = min(10000, len(X_train))
    X_train = X_train.iloc[:sample_size]
    y_train = y_train.iloc[:sample_size]
    
    print(f"âœ… Training: {X_train.shape}, Testing: {X_test.shape}")
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model_type = 'xgboost' if XGBOOST_AVAILABLE else 'random_forest'
    predictor = AdvancedDiabetesPredictor(
        model_type=model_type,
        use_advanced_features=True
    )
    
    predictor.train(X_train, y_train, use_cv=True)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    results = predictor.evaluate(X_test, y_test)
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    predictor.save()
    
    # Ø§Ø®ØªØ¨Ø§Ø± SHAP
    print("\n" + "="*80)
    print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± SHAP Explanation")
    print("="*80)
    
    sample_patient = X_test.iloc[0].to_dict()
    explanation = predictor.explain_prediction(sample_patient)
    
    print(f"\nâœ… Ø§Ù„ØªÙ†Ø¨Ø¤: {'Ø³ÙƒØ±ÙŠ' if explanation['prediction'] == 1 else 'Ù„Ø§ ÙŠÙˆØ¬Ø¯'}")
    print(f"âœ… Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„: {explanation['probability']:.2%}")
    
    if 'shap_explanation' in explanation:
        print(f"\nğŸ” Ø£Ù‡Ù… Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¤Ø«Ø±Ø©:")
        for feat in explanation['shap_explanation']:
            if isinstance(feat, dict):
                print(f"   - {feat['feature']}: {feat['impact']} ({feat['shap_value']:.4f})")
    
    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
